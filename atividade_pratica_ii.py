# -*- coding: utf-8 -*-
"""atividade_pratica_ii.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/159bmaUCa8QPpIEkGD_Uz2HNp8xgcDMSo

# Atividade Prática II - Treinamento e Validação de Modelos de RL

**Aluno:** Felipe Eduardo Gomes

**Disciplina:** Reinforcement Learning - Turma II

**Data:** 21/08/2021



Neste trabalho vamos aplicar `Gym`, `Stable-Baselines3` e `RL Baselines Zoo` para lidar com o treinamento e validação de problemas de aprendizado por reforço. Sua tarefa é:

1. Selecionar um cenário da biblioteca `Gym` de sua preferência, desde que este cenário também seja contemplado pelos modelos disponibilizados na `rl baselines zoo`;<br>
R: Para o trabalho, escolhi utilizar o LunarLander-v2.<br><br>

2. Selecionar três algoritmos das biblioteca `Stable-baselines3` para resolver esse problema. Pesquise na documentação da biblioteca quais são os algoritmos mais adequados para o ambiente escolhido e justifique a sua escolha.<br>
R: Foram escolhidos os seguintes algoritmos: POO, A2C e DQN.<br><br>

3. Realize o treinamento de cada um dos três modelos ---você pode ajustar os parâmetros do modelos, se achar necessário--- e salve os modelos em disco.<br>
R: Os três modelos treinados estão na pasta "models".<br><br>

4. De posse dos modelos treinados e salvos, carregue-os e avalie-os por 10 episódios. Apresente os resultados médios e gere a curva de recompensa acumulada disponibilizada pelo `TensorBoard`.<br>
R: Os resultados e os gráficos foram gerados na pasta "tensorboard".<br><br>

5. Compare os resultados dos modelos treinados com os resultados obtidos por modelo(s) existentes no `RL Baselines Zoo` para o cenário escolhido.<br>
R: Os resultados do treinamento foram gerados na pasta "tensorboard".<br><br>

6. Gere um vídeo do melhor modelo que você treinou e do modelo escolhido na `RL Baselines Zoo`. Verifique a documentação de cada biblioteca sobre a criação do vídeo e visualização em Notebooks.<br>
R: Os vídeos foram gerados estão disponíveis na pasta "videos".<br><br>


* **Data de entrega:** 04/09/2021
* **Local de envio:** AVA.
* **Tipo de documento:** Notebook (`.ipynb`).

**Gráficos do Tensorboard com os resultados do treinamento e do RL Baselines Zoo**
![graficos_tensorboard.png](attachment:graficos_tensorboard.png)

**GIF do modelo A2C treinado**
![LunarLander-v2-A2C.gif](attachment:LunarLander-v2-A2C.gif)

**GIF do modelo DQN treinado**
![LunarLander-v2-DQN.gif](attachment:LunarLander-v2-DQN.gif)

**GIF do modelo PPO treinado**
![LunarLander-v2-PPO.gif](attachment:LunarLander-v2-PPO.gif)
"""

import gym

from stable_baselines3 import DQN
from stable_baselines3 import PPO
from stable_baselines3 import A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback

import time

# Create environment
scenario = 'LunarLander-v2'
env = gym.make(scenario)

from PIL import Image, ImageDraw, ImageFont

def save_frames_as_gif(frames, path='./', filename='gym_animation', label='sample'):
    images = []

    font = ImageFont.truetype('arial')

    for index, frame in enumerate(frames):
        image = Image.fromarray(frame)
        title = f'{label}-{index}'
        draw = ImageDraw.Draw(image)
        width, height = image.size
        text_w, text_h = draw.textsize(title, font)
        draw.text((1, height-text_h-1),title,(255,255,255),font=font)
        images.append(image)

    # loop=0: loop forever, duration=1: play each frame for 1ms
    images[0].save(
        f'{path}{filename}-{label}.gif', save_all=True, append_images=images[1:], loop=0, duration=1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Treina e salva os modelos
# from stable_baselines3 import PPO, A2C, DQN
# 
# PPO('MlpPolicy', env, tensorboard_log=f"./tensorboard/{scenario}-PPO/") \
#     .learn(10000, tb_log_name="first_run").save(f'models/{scenario}-PPO')
# 
# A2C('MlpPolicy', env, tensorboard_log=f"./tensorboard/{scenario}-A2C/") \
#     .learn(10000, tb_log_name="first_run").save(f'models/{scenario}-A2C')
# 
# DQN('MlpPolicy', env, tensorboard_log=f"./tensorboard/{scenario}-DQN/") \
#     .learn(10000, tb_log_name="first_run").save(f'models/{scenario}-DQN')

def evaluate(algoritmo, episodes=500):
  env = gym.make(scenario)

  nome_algoritmo = algoritmo.__name__
  trained_model = algoritmo.load(f"models/{scenario}-{nome_algoritmo}", env=env)

  # Evaluate the agent
  mean_reward, std_reward = evaluate_policy(trained_model, trained_model.get_env(), n_eval_episodes=10)
  print(f"{scenario}-{nome_algoritmo} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")  
  evaluate
    
  frames = []
  obs = env.reset()
  for i in range(episodes):
      action, _states = trained_model.predict(obs, deterministic=True)
      obs, reward, done, info = env.step(action)
      
      env.render()
      time.sleep(0.0003)
        
      rgb_observation = env.render(mode = 'rgb_array') 
      frames.append(rgb_observation)

      if done:
        obs = env.reset()

  env.close()

  save_frames_as_gif(frames, 'images/', scenario, nome_algoritmo)

evaluate(PPO)
evaluate(A2C)
evaluate(DQN)

#RL Baselines3 Zoo - avaliação via TensorBoard DQN
!python train.py --algo dqn --env LunarLander-v2 --verbose 0 --tensorboard-log ../tensorboard/

#RL Baselines3 Zoo - avaliação via TensorBoard PPO
!python train.py --algo ppo --env LunarLander-v2 --verbose 0 --tensorboard-log ../tensorboard/

#RL Baselines3 Zoo - avaliação via TensorBoard A2C
!python train.py --algo a2c --env LunarLander-v2 --verbose 0 --tensorboard-log ../tensorboard/

# Commented out IPython magic to ensure Python compatibility.
# %cd ../

# Commented out IPython magic to ensure Python compatibility.
# %cd rl-baselines3-zoo/
!python -m utils.record_training --algo ppo --env LunarLander-v2 -n 1000 -f logs --deterministic --gif